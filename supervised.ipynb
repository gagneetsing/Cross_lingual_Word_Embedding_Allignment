{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading embeddings from Fastext and unzipping**"
      ],
      "metadata": {
        "id": "j545fhrvEa4J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG5UB2NvEVNN",
        "outputId": "c25701fc-1e73-4816-96df-df2d51ee341c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz...\n",
            "Downloaded ./fasttext_embeddings/cc.en.300.vec.gz\n",
            "Downloading from https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz...\n",
            "Downloaded ./fasttext_embeddings/cc.hi.300.vec.gz\n",
            "Extracting ./fasttext_embeddings/cc.en.300.vec.gz...\n",
            "Extracted to ./fasttext_embeddings/cc.en.300.vec\n",
            "Extracting ./fasttext_embeddings/cc.hi.300.vec.gz...\n",
            "Extracted to ./fasttext_embeddings/cc.hi.300.vec\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Function to download files\n",
        "def download_file(url, dest_path):\n",
        "    print(f\"Downloading from {url}...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(dest_path, 'wb') as file:\n",
        "        shutil.copyfileobj(response.raw, file)\n",
        "    print(f\"Downloaded {dest_path}\")\n",
        "\n",
        "# Function to extract gzip files (FastText vectors are usually .vec.gz)\n",
        "def extract_gzip(source_path, dest_path):\n",
        "    print(f\"Extracting {source_path}...\")\n",
        "    with gzip.open(source_path, 'rb') as f_in:\n",
        "        with open(dest_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    print(f\"Extracted to {dest_path}\")\n",
        "\n",
        "# URLs for FastText embeddings (English and Hindi)\n",
        "english_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n",
        "hindi_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\"\n",
        "\n",
        "# Paths to save the downloaded and extracted files\n",
        "download_dir = \"./fasttext_embeddings/\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# English embeddings paths\n",
        "en_gzip_path = os.path.join(download_dir, \"cc.en.300.vec.gz\")\n",
        "en_vec_path = os.path.join(download_dir, \"cc.en.300.vec\")\n",
        "\n",
        "# Hindi embeddings paths\n",
        "hi_gzip_path = os.path.join(download_dir, \"cc.hi.300.vec.gz\")\n",
        "hi_vec_path = os.path.join(download_dir, \"cc.hi.300.vec\")\n",
        "\n",
        "# Download English and Hindi embeddings\n",
        "download_file(english_url, en_gzip_path)\n",
        "download_file(hindi_url, hi_gzip_path)\n",
        "\n",
        "# Extract the downloaded .gz files\n",
        "extract_gzip(en_gzip_path, en_vec_path)\n",
        "extract_gzip(hi_gzip_path, hi_vec_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading the bilingual dictionary english to hindi**"
      ],
      "metadata": {
        "id": "vnlgYcFjE5NJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnMlR1rIEVNQ",
        "outputId": "f5cbc23c-58f6-4aeb-b9de-bfe97fc98fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt...\n",
            "Downloaded bilingual dictionary to ./muse_bilingual_dictionaries/en-hi.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def download_bilingual_dictionary(url, dest_path):\n",
        "    print(f\"Downloading from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    with open(dest_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"Downloaded bilingual dictionary to {dest_path}\")\n",
        "\n",
        "bilingual_dict_url = \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\"\n",
        "\n",
        "download_dir = \"./muse_bilingual_dictionaries/\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "en_hi_dict_path = os.path.join(download_dir, \"en-hi.txt\")\n",
        "\n",
        "download_bilingual_dictionary(bilingual_dict_url, en_hi_dict_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the embeddings of each linguals, setting the limit to 100000 as said in the provided documentation**"
      ],
      "metadata": {
        "id": "oRkjWKAEFGey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VqtaDVleEVNR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_fasttext_embeddings(file_path, limit=100000):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0:\n",
        "                continue  # Skip the first line with metadata\n",
        "            tokens = line.rstrip().split(' ')\n",
        "            word = tokens[0]\n",
        "            vector = np.asarray(tokens[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "            if len(embeddings) >= limit:\n",
        "                break\n",
        "    return embeddings\n",
        "\n",
        "english_embeddings = load_fasttext_embeddings(\"./fasttext_embeddings/cc.en.300.vec\", limit=100000)\n",
        "hindi_embeddings = load_fasttext_embeddings(\"./fasttext_embeddings/cc.hi.300.vec\", limit=100000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the bilingual dictionary**"
      ],
      "metadata": {
        "id": "pc5z_ocjFpKI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dgchP445EVNS"
      },
      "outputs": [],
      "source": [
        "def load_bilingual_dictionary(file_path):\n",
        "    word_pairs = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            word_pairs.append((en_word, hi_word))\n",
        "    return word_pairs\n",
        "\n",
        "bilingual_dict = load_bilingual_dictionary(\"./muse_bilingual_dictionaries/en-hi.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_bilingual_lexicon(bilingual_dict, n=5000):\n",
        "    return bilingual_dict[:n]\n",
        "\n",
        "# Example usage\n",
        "bilingual_lexicon_5k = extract_bilingual_lexicon(bilingual_dict, 5000)\n",
        "bilingual_lexicon_10k = extract_bilingual_lexicon(bilingual_dict, 10000)\n",
        "bilingual_lexicon_20k = extract_bilingual_lexicon(bilingual_dict, 20000)\n"
      ],
      "metadata": {
        "id": "GnVy3qHK2aWS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This code section makes the alignment of the english and the hindi embeddings vectors by fetching the the vector associated with en_word and hi_word from there respective embeddings**"
      ],
      "metadata": {
        "id": "YmQ6ODHOqK4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdhcEkJ7EVNS",
        "outputId": "de7f3d9c-0834-4336-dcb1-a2f901fee957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alignment Matrices: X shape = (15314, 300), Y shape = (15314, 300)\n"
          ]
        }
      ],
      "source": [
        "def create_alignment_matrices(bilingual_dict, en_embeddings, hi_embeddings):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for en_word, hi_word in bilingual_dict:\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            X.append(en_embeddings[en_word])\n",
        "            Y.append(hi_embeddings[hi_word])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "X, Y = create_alignment_matrices(bilingual_dict, english_embeddings, hindi_embeddings)\n",
        "print(f\"Alignment Matrices: X shape = {X.shape}, Y shape = {Y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This code performs Procrustes alignment to align word embeddings (English to hindi) using linear transformation**"
      ],
      "metadata": {
        "id": "qv_b6Fscr4m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It computes the optimal linear transformatiom matrix W that aligns X (English embeddings) to Y (Hindi Embeddings) as closely as possible, whereas W is the transformation matrix. This matrix is then used to transform X by computing aligned_X = X.dot(W), aligning the Engish embeddings to the Hindi space."
      ],
      "metadata": {
        "id": "3HWeYg7Bsbft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZWAabLAvEVNS"
      },
      "outputs": [],
      "source": [
        "from scipy.linalg import orthogonal_procrustes\n",
        "\n",
        "def procrustes_alignment(X, Y):\n",
        "    W, _ = orthogonal_procrustes(X, Y)\n",
        "    return W\n",
        "\n",
        "# Use Procrustes alignment for each lexicon size\n",
        "def perform_alignment(bilingual_lexicon, en_embeddings, hi_embeddings):\n",
        "    X, Y = create_alignment_matrices(bilingual_lexicon, en_embeddings, hi_embeddings)\n",
        "    W = procrustes_alignment(X, Y)\n",
        "    aligned_X = X.dot(W)\n",
        "    return aligned_X, Y, W\n",
        "\n",
        "aligned_X_5k, Y_5k, W_5k = perform_alignment(bilingual_lexicon_5k, english_embeddings, hindi_embeddings)\n",
        "aligned_X_10k, Y_10k, W_10k = perform_alignment(bilingual_lexicon_10k, english_embeddings, hindi_embeddings)\n",
        "aligned_X_20k, Y_20k, W_20k = perform_alignment(bilingual_lexicon_20k, english_embeddings, hindi_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It performs English to Hindi word translation using aligned word embeddings and cosine similarity.\n",
        "1. It first checks if the English word exists in en_embeddings. If not, it returns an empty list\n",
        "2. If the word exists, it retrieves the embeddings and aligns it using the transformation matrix W obtained from Procrustes alignment.\n",
        "3. The aligned word vector is compared to all Hindi embeddings using cosine_similarity. The higher the similarity, the closer the words are semantically.\n",
        "4. It identifies the top-k most similar Hindi words by sorting the cosine similarity values and retreiving the correspondig Hindi words from the hi_embeddings."
      ],
      "metadata": {
        "id": "ch1eDHnTubz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU-Hw3_mEVNT",
        "outputId": "75829363-2c05-4d44-e40f-82bb1788c6da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Hindi translations for 'world': [('दुनिया', 0.70340943), ('देश', 0.59056157), ('विश्व', 0.5696856), ('दुनियां', 0.5615448), ('संसार', 0.54642093)]\n",
            "Top Hindi translations for 'world': [('दुनिया', 0.70340943), ('देश', 0.59056157), ('विश्व', 0.5696856), ('दुनियां', 0.5615448), ('संसार', 0.54642093)]\n",
            "Top Hindi translations for 'world': [('दुनिया', 0.70340943), ('देश', 0.59056157), ('विश्व', 0.5696856), ('दुनियां', 0.5615448), ('संसार', 0.54642093)]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def translate_word(word, en_embeddings, aligned_X, hi_embeddings, top_k=5):\n",
        "    if word not in en_embeddings:\n",
        "        return []\n",
        "\n",
        "    word_vec = en_embeddings[word].dot(W)\n",
        "    similarities = cosine_similarity([word_vec], list(hi_embeddings.values()))[0]\n",
        "    closest_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    hindi_words = list(hi_embeddings.keys())\n",
        "    return [(hindi_words[i], similarities[i]) for i in closest_indices]\n",
        "\n",
        "translation_5k = translate_word(\"world\", english_embeddings, aligned_X_5k, hindi_embeddings)\n",
        "translation_10k = translate_word(\"world\", english_embeddings, aligned_X_10k, hindi_embeddings)\n",
        "translation_20k = translate_word(\"world\", english_embeddings, aligned_X_20k, hindi_embeddings)\n",
        "\n",
        "print(f\"Top Hindi translations for 'world': {translation_5k}\")\n",
        "print(f\"Top Hindi translations for 'world': {translation_10k}\")\n",
        "print(f\"Top Hindi translations for 'world': {translation_20k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This code evaluates the precision of English-to-Hindi word translations using word embeddings. Here's the step-by-step explanation:**\n",
        "\n",
        "\n",
        "1. correct_at_1 counts cases where the correct translation is the top-1 result.\n",
        "2. correct_at_5 counts cases where the correct translation is among the top-5 results.\n",
        "3. total tracks how many word pairs are evaluated.\n",
        "\n",
        "4. For each word pair (en_word, hi_word), it retrieves the translations using the translate_word function and checks if the correct Hindi word appears in the top-k results.\n",
        "Calculate Precision:\n",
        "\n",
        "5. precision_at_1 is the proportion of correct top-1 translations.\n",
        "6. precision_at_5 is the proportion of correct top-5 translations."
      ],
      "metadata": {
        "id": "qP55C0COxOsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def translate_word(word, en_embeddings, W, hi_embeddings, top_k=5):\n",
        "    if word not in en_embeddings:\n",
        "        return []\n",
        "\n",
        "    word_vec = en_embeddings[word].dot(W)\n",
        "    similarities = cosine_similarity([word_vec], list(hi_embeddings.values()))[0]\n",
        "    closest_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    hindi_words = list(hi_embeddings.keys())\n",
        "    return [(hindi_words[i], similarities[i]) for i in closest_indices]\n",
        "\n",
        "def evaluate_precision(bilingual_dict, en_embeddings, hi_embeddings, W, top_k=5):\n",
        "    correct_at_1 = 0\n",
        "    correct_at_5 = 0\n",
        "    total = 0\n",
        "\n",
        "    for en_word, hi_word in bilingual_dict:\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            translations = translate_word(en_word, en_embeddings, W, hi_embeddings, top_k=top_k)\n",
        "            top_words = [t[0] for t in translations]\n",
        "\n",
        "            if hi_word == top_words[0]:\n",
        "                correct_at_1 += 1\n",
        "            if hi_word in top_words:\n",
        "                correct_at_5 += 1\n",
        "            total += 1\n",
        "\n",
        "    precision_at_1 = correct_at_1 / total\n",
        "    precision_at_5 = correct_at_5 / total\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "# Evaluate with different lexicon sizes\n",
        "precision_at_1_5k, precision_at_5_5k = evaluate_precision(bilingual_dict, english_embeddings, hindi_embeddings, W_5k, top_k=5)\n",
        "precision_at_1_10k, precision_at_5_10k = evaluate_precision(bilingual_dict, english_embeddings, hindi_embeddings, W_10k, top_k=5)\n",
        "precision_at_1_20k, precision_at_5_20k = evaluate_precision(bilingual_dict, english_embeddings, hindi_embeddings, W_20k, top_k=5)\n",
        "\n",
        "print(f\"Precision@1 (5k): {precision_at_1_5k:.4f}, Precision@5 (5k): {precision_at_5_5k:.4f}\")\n",
        "print(f\"Precision@1 (10k): {precision_at_1_10k:.4f}, Precision@5 (10k): {precision_at_5_10k:.4f}\")\n",
        "print(f\"Precision@1 (20k): {precision_at_1_20k:.4f}, Precision@5 (20k): {precision_at_5_20k:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i8-pePd3-Qz",
        "outputId": "7350c862-bbad-470e-90a7-25fe73c8fd84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@1 (5k): 0.2867, Precision@5 (5k): 0.5624\n",
            "Precision@1 (10k): 0.3328, Precision@5 (10k): 0.6233\n",
            "Precision@1 (20k): 0.3630, Precision@5 (20k): 0.6544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJYcqip-EVNV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}