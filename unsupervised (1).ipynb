{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# full code for unsupevised with adversarial training"
      ],
      "metadata": {
        "id": "1FVGDedyi42p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsRsOpnAqM0J",
        "outputId": "b72cb992-e1d0-416f-d5f0-2643f400cdf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "./fasttext_embeddings/cc.en.300.vec.gz already exists.\n",
            "./fasttext_embeddings/cc.hi.300.vec.gz already exists.\n",
            "./fasttext_embeddings/cc.en.300.vec already exists.\n",
            "./fasttext_embeddings/cc.hi.300.vec already exists.\n",
            "Loading English embeddings...\n",
            "Loading Hindi embeddings...\n",
            "Number of common words: 35732\n",
            "CSLS Calculated.\n",
            "Precision at 1: 0.5493\n",
            "Precision at 5: 0.5493\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Function to download files\n",
        "def download_file(url, dest_path):\n",
        "    if not os.path.exists(dest_path):\n",
        "        print(f\"Downloading from {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(dest_path, 'wb') as file:\n",
        "            shutil.copyfileobj(response.raw, file)\n",
        "        print(f\"Downloaded {dest_path}\")\n",
        "    else:\n",
        "        print(f\"{dest_path} already exists.\")\n",
        "\n",
        "# Function to extract gzip files\n",
        "def extract_gzip(source_path, dest_path):\n",
        "    if not os.path.exists(dest_path):\n",
        "        print(f\"Extracting {source_path}...\")\n",
        "        with gzip.open(source_path, 'rb') as f_in:\n",
        "            with open(dest_path, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "        print(f\"Extracted to {dest_path}\")\n",
        "    else:\n",
        "        print(f\"{dest_path} already exists.\")\n",
        "\n",
        "# URLs for FastText embeddings (English and Hindi)\n",
        "english_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n",
        "hindi_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\"\n",
        "\n",
        "# Paths to save the downloaded and extracted files\n",
        "download_dir = \"./fasttext_embeddings/\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# English embeddings paths\n",
        "en_gzip_path = os.path.join(download_dir, \"cc.en.300.vec.gz\")\n",
        "en_vec_path = os.path.join(download_dir, \"cc.en.300.vec\")\n",
        "\n",
        "# Hindi embeddings paths\n",
        "hi_gzip_path = os.path.join(download_dir, \"cc.hi.300.vec.gz\")\n",
        "hi_vec_path = os.path.join(download_dir, \"cc.hi.300.vec\")\n",
        "\n",
        "# Download English and Hindi embeddings\n",
        "download_file(english_url, en_gzip_path)\n",
        "download_file(hindi_url, hi_gzip_path)\n",
        "\n",
        "# Extract the downloaded .gz files\n",
        "extract_gzip(en_gzip_path, en_vec_path)\n",
        "extract_gzip(hi_gzip_path, hi_vec_path)\n",
        "\n",
        "# Function to load FastText embeddings from a .vec file\n",
        "def load_fasttext_embeddings(file_path, limit=200000):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        next(f)  # Skip the header\n",
        "        for i, line in enumerate(f):\n",
        "            tokens = line.rstrip().split(' ')\n",
        "            word = tokens[0]\n",
        "            vector = np.asarray(tokens[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "            if len(embeddings) >= limit:\n",
        "                break\n",
        "    return embeddings\n",
        "\n",
        "# Load English and Hindi embeddings\n",
        "print(\"Loading English embeddings...\")\n",
        "english_embeddings = load_fasttext_embeddings(en_vec_path)\n",
        "print(\"Loading Hindi embeddings...\")\n",
        "hindi_embeddings = load_fasttext_embeddings(hi_vec_path)\n",
        "\n",
        "# Filter embeddings to common words only\n",
        "common_words = set(english_embeddings.keys()).intersection(set(hindi_embeddings.keys()))\n",
        "print(f\"Number of common words: {len(common_words)}\")\n",
        "\n",
        "# Convert embeddings to matrices\n",
        "en_embeddings = np.array([english_embeddings[word] for word in common_words])\n",
        "hi_embeddings = np.array([hindi_embeddings[word] for word in common_words])\n",
        "\n",
        "# Normalize embeddings\n",
        "def normalize_embeddings(embeddings):\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    return embeddings / norms\n",
        "\n",
        "en_embeddings = normalize_embeddings(en_embeddings)\n",
        "hi_embeddings = normalize_embeddings(hi_embeddings)\n",
        "\n",
        "# Adversarial Training Setup\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 2048)\n",
        "        self.fc3 = nn.Linear(2048, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky_relu(self.fc1(x))\n",
        "        x = self.leaky_relu(self.fc2(x))\n",
        "        return torch.sigmoid(self.fc3(x))\n",
        "\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "        self.fc2 = nn.Linear(output_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = en_embeddings.shape[1]\n",
        "output_dim = hi_embeddings.shape[1]\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "\n",
        "# Create instances of the Discriminator and Mapping Network\n",
        "discriminator = Discriminator(input_dim).to(device)\n",
        "mapping_network = MappingNetwork(input_dim, output_dim).to(device)\n",
        "\n",
        "# Optimizers\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "m_optimizer = optim.Adam(mapping_network.parameters(), lr=lr)\n",
        "\n",
        "# Adversarial Training\n",
        "# Adversarial Training\n",
        "def adversarial_training(src_emb, tgt_emb, mapping_net, discriminator, num_epochs=10, batch_size=128, lr=0.001):\n",
        "    # Optimizers\n",
        "    mapping_optimizer = optim.Adam(mapping_net.parameters(), lr=lr)\n",
        "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "    # Loss function\n",
        "    adversarial_loss = nn.BCELoss()\n",
        "\n",
        "    # Convert embeddings to PyTorch tensors\n",
        "    src_emb = torch.tensor(src_emb, dtype=torch.float32)\n",
        "    tgt_emb = torch.tensor(tgt_emb, dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(0, len(src_emb), batch_size):\n",
        "            # Sample a batch of source and target embeddings\n",
        "            src_batch = src_emb[i:i+batch_size]\n",
        "            tgt_batch = tgt_emb[i:i+batch_size]\n",
        "\n",
        "            # Create labels for discriminator training\n",
        "            src_labels = torch.zeros(src_batch.size(0))\n",
        "            tgt_labels = torch.ones(tgt_batch.size(0))\n",
        "\n",
        "            # Train discriminator\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            src_pred = discriminator(src_batch)\n",
        "            tgt_pred = discriminator(tgt_batch)\n",
        "            loss_d = adversarial_loss(src_pred, src_labels) + adversarial_loss(tgt_pred, tgt_labels)\n",
        "            loss_d.backward()\n",
        "            discriminator_optimizer.step()\n",
        "\n",
        "            # Train mapping to fool the discriminator\n",
        "            mapping_optimizer.zero_grad()\n",
        "            mapped_src_batch = mapping_net(src_batch)\n",
        "            src_pred = discriminator(mapped_src_batch)\n",
        "            loss_g = adversarial_loss(src_pred, tgt_labels)  # Fool the discriminator\n",
        "            loss_g.backward()\n",
        "            mapping_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Discriminator Loss: {loss_d.item()}, Generator Loss: {loss_g.item()}\")\n",
        "\n",
        "    # Return the learned mapping\n",
        "    return mapping_net\n",
        "\n",
        "\n",
        "# Procrustes Refinement\n",
        "def procrustes_alignment(source_embeddings, target_embeddings):\n",
        "    \"\"\"Align source embeddings to target embeddings using Procrustes analysis.\"\"\"\n",
        "    source_centered = source_embeddings - np.mean(source_embeddings, axis=0)\n",
        "    target_centered = target_embeddings - np.mean(target_embeddings, axis=0)\n",
        "    covariance = np.dot(source_centered.T, target_centered)\n",
        "    U, _, Vt = np.linalg.svd(covariance)\n",
        "    rotation_matrix = np.dot(U, Vt)\n",
        "    refined_source = np.dot(source_embeddings, rotation_matrix)\n",
        "    return refined_source\n",
        "\n",
        "# Refine the mapping using Procrustes\n",
        "refined_en_embeddings = procrustes_alignment(en_embeddings, hi_embeddings)\n",
        "\n",
        "# Function to calculate Cross-Domain Similarity Local Scaling (CSLS)\n",
        "def calculate_csls(source_embeddings, target_embeddings, k=5):\n",
        "    nbrs = NearestNeighbors(n_neighbors=k, metric='cosine').fit(target_embeddings)\n",
        "\n",
        "    source_csls = np.mean(nbrs.kneighbors(source_embeddings, return_distance=False), axis=1)\n",
        "    target_csls = np.mean(nbrs.kneighbors(target_embeddings, return_distance=False), axis=1)\n",
        "\n",
        "    return source_csls, target_csls\n",
        "\n",
        "# Calculate CSLS\n",
        "source_csls, target_csls = calculate_csls(refined_en_embeddings, hi_embeddings)\n",
        "print(\"CSLS Calculated.\")\n",
        "\n",
        "# Function to calculate Precision at 1 (P@1) and Precision at 5 (P@5)\n",
        "def calculate_precision(source_embeddings, target_embeddings, common_words, k=5):\n",
        "    # Create a mapping from word to its index for both languages\n",
        "    word_list = list(common_words)\n",
        "    en_word_to_index = {word: idx for idx, word in enumerate(word_list)}\n",
        "    hi_word_to_index = {word: idx for idx, word in enumerate(word_list)}\n",
        "\n",
        "    # Fit Nearest Neighbors on target embeddings\n",
        "    nbrs = NearestNeighbors(n_neighbors=k, metric='cosine').fit(target_embeddings)\n",
        "\n",
        "    # Initialize counters for correct predictions\n",
        "    correct_at_1 = 0\n",
        "    correct_at_5 = 0\n",
        "\n",
        "    for idx, en_word in enumerate(word_list):\n",
        "        hi_word = word_list[idx]  # Assuming a one-to-one mapping in the same index\n",
        "        # Get the English word vector\n",
        "        en_vector = source_embeddings[en_word_to_index[en_word]]\n",
        "        # Find the k nearest neighbors in the target embeddings\n",
        "        neighbors = nbrs.kneighbors([en_vector], return_distance=False)[0]\n",
        "\n",
        "        # Check if the true translation is in the nearest neighbors\n",
        "        if hi_word_to_index[hi_word] in neighbors:\n",
        "            correct_at_1 += 1\n",
        "        if any(hi_word_to_index[hi_word] == neighbor for neighbor in neighbors):\n",
        "            correct_at_5 += 1\n",
        "\n",
        "    precision_at_1 = correct_at_1 / len(common_words)\n",
        "    precision_at_5 = correct_at_5 / len(common_words)\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "# Generate ground truth pairs from common words\n",
        "ground_truth_pairs = [(word, word) for word in common_words]\n",
        "\n",
        "# Calculate precision\n",
        "precision_at_1, precision_at_5 = calculate_precision(refined_en_embeddings, hi_embeddings, ground_truth_pairs)\n",
        "print(f\"Precision at 1: {precision_at_1:.4f}\")\n",
        "print(f\"Precision at 5: {precision_at_5:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AlCQTuyryHUL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q1gKMavjpc2L"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-4AIMO80qd5I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzxIkRCgqwed"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}